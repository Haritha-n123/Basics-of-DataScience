{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c6d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "ledger = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_LDGR_Bloomberg_251125 1.csv\")\n",
    "statement = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_STMT_Calypso_251125 1.csv\")\n",
    "\n",
    "# Parse dates properly - use format='mixed' to handle varying formats\n",
    "ledger[\"Trade_Date\"] = pd.to_datetime(ledger[\"Trade_Date\"], format=\"mixed\", dayfirst=True)\n",
    "statement[\"Trade_Date\"] = pd.to_datetime(statement[\"Trade_Date\"], format=\"mixed\", dayfirst=True)\n",
    "\n",
    "# Normalize to date only for matching (remove time component)\n",
    "ledger[\"Trade_Date\"] = ledger[\"Trade_Date\"].dt.date\n",
    "statement[\"Trade_Date\"] = statement[\"Trade_Date\"].dt.date\n",
    "\n",
    "ledger[\"source_type\"] = \"ledger\"\n",
    "statement[\"source_type\"] = \"statement\"\n",
    "\n",
    "# Normalize signage\n",
    "def normalize_signage(x):\n",
    "    return x.strip().upper()\n",
    "\n",
    "ledger[\"Signage\"] = ledger[\"Signage\"].apply(normalize_signage)\n",
    "statement[\"Signage\"] = statement[\"Signage\"].apply(normalize_signage)\n",
    "\n",
    "# Create signed amount\n",
    "def signed_amount(row):\n",
    "    if row[\"Signage\"] in [\"DR\", \"D\"]:\n",
    "        return -row[\"Amount1\"]\n",
    "    elif row[\"Signage\"] in [\"CR\", \"C\"]:\n",
    "        return row[\"Amount1\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "ledger[\"signed_amount\"] = ledger.apply(signed_amount, axis=1)\n",
    "statement[\"signed_amount\"] = statement.apply(signed_amount, axis=1)\n",
    "\n",
    "# For matching, use absolute amounts (signage differences are normal between systems)\n",
    "ledger[\"abs_amount\"] = ledger[\"signed_amount\"].abs()\n",
    "statement[\"abs_amount\"] = statement[\"signed_amount\"].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5befd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, HfArgumentParser, pipeline, logging\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304477ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_confidence(ledger_row, statement_row):\n",
    "    \"\"\"\n",
    "    Calculate confidence score based on match quality.\n",
    "    Perfect match = 1.0, deductions for minor discrepancies.\n",
    "    \"\"\"\n",
    "    confidence = 1.0\n",
    "    \n",
    "    # Check for exact matches on key fields\n",
    "    if ledger_row[\"ISIN_CUSIP\"] != statement_row[\"ISIN_CUSIP\"]:\n",
    "        confidence -= 0.15\n",
    "    \n",
    "    if ledger_row[\"Trade_Date\"] != statement_row[\"Trade_Date\"]:\n",
    "        confidence -= 0.10\n",
    "    \n",
    "    if ledger_row[\"Currency\"] != statement_row[\"Currency\"]:\n",
    "        confidence -= 0.20\n",
    "    \n",
    "    if ledger_row[\"Quantity\"] != statement_row[\"Quantity\"]:\n",
    "        confidence -= 0.15\n",
    "    \n",
    "    # Check if amounts match (should match after abs normalization)\n",
    "    if abs(ledger_row[\"signed_amount\"] - statement_row[\"signed_amount\"]) > 0.01:\n",
    "        confidence -= 0.20\n",
    "    \n",
    "    # Check references\n",
    "    if ledger_row[\"Ref1\"] != statement_row[\"Ref1\"]:\n",
    "        confidence -= 0.05\n",
    "    \n",
    "    if ledger_row[\"Ref2\"] != statement_row[\"Ref2\"]:\n",
    "        confidence -= 0.05\n",
    "    \n",
    "    # Ensure confidence is between 0 and 1\n",
    "    return max(0.0, min(1.0, confidence))\n",
    "\n",
    "def create_finetune_sample(ledger_row, statement_row):\n",
    "    confidence = calculate_confidence(ledger_row, statement_row)\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "Ledger Transaction:\n",
    "{ledger_row.to_dict()}\n",
    "\n",
    "Statement Transaction:\n",
    "{statement_row.to_dict()}\n",
    "\n",
    "Task:\n",
    "Check if these transactions reconcile.\n",
    "Return:\n",
    "- matched (true/false)\n",
    "- reason\n",
    "- confidence (0-1)\n",
    "\"\"\"\n",
    "\n",
    "    assistant_response = {\n",
    "        \"matched\": True,\n",
    "        \"ledger_source\": ledger_row[\"Source\"],\n",
    "        \"statement_source\": statement_row[\"Source\"],\n",
    "        \"confidence\": round(confidence, 2),\n",
    "        \"reason\": (\n",
    "            \"Amounts, currency, trade date, transaction code, quantity \"\n",
    "            \"and references match across ledger and statement. \"\n",
    "            \"Signage difference handled via accounting rules.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial reconciliation expert.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(assistant_response)}\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75215900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched rows: 903\n"
     ]
    }
   ],
   "source": [
    "match_keys = [\n",
    "    \"Trade_Date\",\n",
    "    \"Currency\",\n",
    "    \"Tran_code\",\n",
    "    \"Quantity\",\n",
    "    \"Ref1\",\n",
    "    \"Ref2\",\n",
    "    \"abs_amount\"  # Use absolute amount for matching\n",
    "]\n",
    "\n",
    "merged = ledger.merge(\n",
    "    statement,\n",
    "    on=match_keys,\n",
    "    suffixes=(\"_ledger\", \"_statement\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Matched rows:\", len(merged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba68bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2003 training examples\n",
      "Sample: {'instruction': \"Analyze the following transaction: {'Source': 'Bloomberg', 'ISIN_CUSIP': 'R2YEXP0DR', 'Trade_Date': '07-05-2022 00:00:00', 'Currency': 'CAD', 'Tran_code': 'DIV', 'Quantity': 111, 'Amount1': 25633.43, 'Amount2': 35268.18, 'Signage': 'DR', 'Ref1': '7GVUPC', 'Ref2': 'A2WAHE', 'Trade_status': 'Confirmed'}\", 'output': 'This is a financial transaction with details: Source: Bloomberg, ISIN_CUSIP: R2YEXP0DR, Trade_Date: 07-05-2022 00:00:00, Currency: CAD, Tran_code: DIV, Quantity: 111, Amount1: 25633.43, Amount2: 35268.18, Signage: DR, Ref1: 7GVUPC, Ref2: A2WAHE, Trade_status: Confirmed', 'text': \"### Instruction:\\nAnalyze the following transaction: {'Source': 'Bloomberg', 'ISIN_CUSIP': 'R2YEXP0DR', 'Trade_Date': '07-05-2022 00:00:00', 'Currency': 'CAD', 'Tran_code': 'DIV', 'Quantity': 111, 'Amount1': 25633.43, 'Amount2': 35268.18, 'Signage': 'DR', 'Ref1': '7GVUPC', 'Ref2': 'A2WAHE', 'Trade_status': 'Confirmed'}\\n### Response:\\nThis is a financial transaction with details: Source: Bloomberg, ISIN_CUSIP: R2YEXP0DR, Trade_Date: 07-05-2022 00:00:00, Currency: CAD, Tran_code: DIV, Quantity: 111, Amount1: 25633.43, Amount2: 35268.18, Signage: DR, Ref1: 7GVUPC, Ref2: A2WAHE, Trade_status: Confirmed\"}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare data for fine-tuning\n",
    "# Convert transaction data into instruction-response format\n",
    "\n",
    "def format_data_for_finetuning(df):\n",
    "    \"\"\"\n",
    "    Convert transaction data into instruction-response pairs for fine-tuning.\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create instruction-response pairs from transaction data\n",
    "        instruction = f\"Analyze the following transaction: {row.to_dict()}\"\n",
    "        response = f\"This is a financial transaction with details: {', '.join([f'{k}: {v}' for k, v in row.to_dict().items()])}\"\n",
    "        \n",
    "        formatted_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"output\": response,\n",
    "            \"text\": f\"### Instruction:\\n{instruction}\\n### Response:\\n{response}\"\n",
    "        })\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Prepare the dataset\n",
    "fine_tune_data = format_data_for_finetuning(df)\n",
    "print(f\"Prepared {len(fine_tune_data)} training examples\")\n",
    "print(\"Sample:\", fine_tune_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f945ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for _, row in merged.iterrows():\n",
    "\n",
    "    ledger_row = {\n",
    "        \"Source\": row[\"Source_ledger\"],\n",
    "        \"ISIN_CUSIP\": row[\"ISIN_CUSIP_ledger\"],\n",
    "        \"Trade_Date\": str(row[\"Trade_Date\"]),\n",
    "        \"Currency\": row[\"Currency\"],\n",
    "        \"Tran_code\": row[\"Tran_code\"],\n",
    "        \"Quantity\": row[\"Quantity\"],\n",
    "        \"signed_amount\": row[\"signed_amount_ledger\"],\n",
    "        \"Ref1\": row[\"Ref1\"],\n",
    "        \"Ref2\": row[\"Ref2\"],\n",
    "        \"Trade_status\": row[\"Trade_status_ledger\"]\n",
    "    }\n",
    "\n",
    "    statement_row = {\n",
    "        \"Source\": row[\"Source_statement\"],\n",
    "        \"ISIN_CUSIP\": row[\"ISIN_CUSIP_statement\"],\n",
    "        \"Trade_Date\": str(row[\"Trade_Date\"]),\n",
    "        \"Currency\": row[\"Currency\"],\n",
    "        \"Tran_code\": row[\"Tran_code\"],\n",
    "        \"Quantity\": row[\"Quantity\"],\n",
    "        \"signed_amount\": row[\"signed_amount_statement\"],\n",
    "        \"Ref1\": row[\"Ref1\"],\n",
    "        \"Ref2\": row[\"Ref2\"],\n",
    "        \"Trade_status\": row[\"Trade_status_statement\"]\n",
    "    }\n",
    "\n",
    "    training_data.append(\n",
    "        create_finetune_sample(\n",
    "            pd.Series(ledger_row),\n",
    "            pd.Series(statement_row)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6374ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to C:\\Users\\HarithaNagamalla\\Downloads\\fine_tune_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Save formatted data to JSON for training\n",
    "import json\n",
    "\n",
    "output_path = r\"C:\\Users\\HarithaNagamalla\\Downloads\\fine_tune_dataset.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    for row in training_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bda4413",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fine_tune_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create dataset from dict\u001b[39;00m\n\u001b[32m      7\u001b[39m train_dataset = Dataset.from_dict({\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: [item[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfine_tune_data\u001b[49m]\n\u001b[32m      9\u001b[39m })\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Model configuration\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'fine_tune_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: Load dataset and tokenizer with proper configuration\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Create dataset from dict\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': [item['text'] for item in fine_tune_data]\n",
    "})\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Quantization configuration for efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer with legacy=False to avoid PyPreTokenizerTypeWrapper error\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True,\n",
    "        legacy=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Legacy=False failed, trying with legacy=True: {e}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True,\n",
    "        legacy=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bb801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HarithaNagamalla\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [40:40<00:00, 813.50s/it]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HarithaNagamalla\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [40:40<00:00, 813.50s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash attention failed, loading with default attention: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HarithaNagamalla\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [40:40<00:00, 813.50s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash attention failed, loading with default attention: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [41:18<00:00, 826.17s/it] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HarithaNagamalla\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 3 files: 100%|██████████| 3/3 [40:40<00:00, 813.50s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash attention failed, loading with default attention: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [41:18<00:00, 826.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 4-bit quantization\n",
      "Model dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load model with quantization\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\"  # Optional: for better performance\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Flash attention failed, loading with default attention: {e}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Set gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False  # Disable cache during training\n",
    "\n",
    "print(\"Model loaded with 4-bit quantization\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a139b21f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m lora_config = LoraConfig(\n\u001b[32m      6\u001b[39m     r=\u001b[32m16\u001b[39m,  \u001b[38;5;66;03m# LoRA rank\u001b[39;00m\n\u001b[32m      7\u001b[39m     lora_alpha=\u001b[32m32\u001b[39m,  \u001b[38;5;66;03m# LoRA alpha for scaling\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Apply LoRA to model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model = get_peft_model(\u001b[43mmodel\u001b[49m, lora_config)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Show trainable parameters\u001b[39;00m\n\u001b[32m     18\u001b[39m trainable_params = \u001b[38;5;28msum\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 5: Configure LoRA for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # LoRA alpha for scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Show trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} / {total_params:,}\")\n",
    "print(f\"Trainable percentage: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac02d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2003/2003 [00:02<00:00, 735.69 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2003/2003 [00:02<00:00, 735.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 2003\n",
      "})\n",
      "Sample token ids shape: [1, 774, 3133, 3112, 28747, 13, 27554, 1374, 272, 2296, 8966, 28747, 12012, 4220, 1869, 464, 28107, 300, 4146, 647]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2003/2003 [00:02<00:00, 735.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 2003\n",
      "})\n",
      "Sample token ids shape: [1, 774, 3133, 3112, 28747, 13, 27554, 1374, 272, 2296, 8966, 28747, 12012, 4220, 1869, 464, 28107, 300, 4146, 647]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "print(f\"Sample token ids shape: {tokenized_dataset[0]['input_ids'][:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac3df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured\n",
      "Output directory: C:\\Users\\HarithaNagamalla\\Downloads\\fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Configure training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\HarithaNagamalla\\Downloads\\fine_tuned_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e25a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Initialize trainer and fine-tune\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal language modeling (not masked)\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting fine-tuning...\")\n",
    "# Uncomment the line below to start training (this will take some time)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save fine-tuned model\n",
    "# After training completes, save the model and adapter\n",
    "def save_fine_tuned_model(model, tokenizer, save_path):\n",
    "    \"\"\"Save the fine-tuned model and tokenizer\"\"\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# Uncomment after training:\n",
    "save_fine_tuned_model(model, tokenizer, r\"C:\\Users\\HarithaNagamalla\\Downloads\\transaction_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Test the fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "def test_fine_tuned_model(model, tokenizer, prompt):\n",
    "    \"\"\"Generate text using the fine-tuned model\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    \n",
    "    result = pipe(prompt)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Analyze the following financial transaction: Amount: 1000, Date: 2025-01-25, Type: Transfer\"\n",
    "\n",
    "# Uncomment to test after training:\n",
    "output = test_fine_tuned_model(model, tokenizer, test_prompt)\n",
    "print(\"Generated output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
