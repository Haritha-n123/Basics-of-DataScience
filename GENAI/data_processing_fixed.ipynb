{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16180748",
   "metadata": {},
   "source": [
    "# Financial Reconciliation Data Processing & Fine-Tuning\n",
    "Complete pipeline: Data loading → Matching → Training data creation → Model fine-tuning → Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb7230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 1: Import Libraries ====================\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b319f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Ledger shape: (1003, 12)\n",
      "Statement shape: (1000, 12)\n",
      "\n",
      "Ledger columns: ['Source', 'ISIN_CUSIP', 'Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'Amount1', 'Amount2', 'Signage', 'Ref1', 'Ref2', 'Trade_status']\n",
      "Statement columns: ['Source', 'ISIN_CUSIP', 'Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'Amount1', 'Amount2', 'Signage', 'Ref1', 'Ref2', 'Trade_status']\n",
      "\n",
      "Data preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 2: Load and Preprocess Data ====================\n",
    "print(\"Loading data...\")\n",
    "ledger = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_LDGR_Bloomberg_251125 1.csv\")\n",
    "statement = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_STMT_Calypso_251125 1.csv\")\n",
    "\n",
    "print(f\"Ledger shape: {ledger.shape}\")\n",
    "print(f\"Statement shape: {statement.shape}\")\n",
    "print(f\"\\nLedger columns: {ledger.columns.tolist()}\")\n",
    "print(f\"Statement columns: {statement.columns.tolist()}\")\n",
    "\n",
    "# Parse dates\n",
    "ledger[\"Trade_Date\"] = pd.to_datetime(ledger[\"Trade_Date\"], format=\"mixed\", dayfirst=True).dt.date\n",
    "statement[\"Trade_Date\"] = pd.to_datetime(statement[\"Trade_Date\"], format=\"mixed\", dayfirst=True).dt.date\n",
    "\n",
    "# Normalize signage\n",
    "def normalize_signage(x):\n",
    "    return x.strip().upper() if pd.notna(x) else \"C\"\n",
    "\n",
    "ledger[\"Signage\"] = ledger[\"Signage\"].apply(normalize_signage)\n",
    "statement[\"Signage\"] = statement[\"Signage\"].apply(normalize_signage)\n",
    "\n",
    "# Create signed amounts\n",
    "def signed_amount(row):\n",
    "    if row[\"Signage\"] in [\"DR\", \"D\"]:\n",
    "        return -row[\"Amount1\"]\n",
    "    elif row[\"Signage\"] in [\"CR\", \"C\"]:\n",
    "        return row[\"Amount1\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "ledger[\"signed_amount\"] = ledger.apply(signed_amount, axis=1)\n",
    "statement[\"signed_amount\"] = statement.apply(signed_amount, axis=1)\n",
    "\n",
    "ledger[\"abs_amount\"] = ledger[\"signed_amount\"].abs()\n",
    "statement[\"abs_amount\"] = statement[\"signed_amount\"].abs()\n",
    "\n",
    "# Fill missing values in reference columns\n",
    "for col in [\"Ref1\", \"Ref2\"]:\n",
    "    if col in ledger.columns:\n",
    "        ledger[col] = ledger[col].fillna(\"UNKNOWN\").astype(str)\n",
    "    if col in statement.columns:\n",
    "        statement[col] = statement[col].fillna(\"UNKNOWN\").astype(str)\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e05c9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching transactions...\n",
      "✓ Including Ref1 in match keys\n",
      "✓ Including Ref2 in match keys\n",
      "\n",
      "Using match keys: ['Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'abs_amount', 'Ref1', 'Ref2']\n",
      "\n",
      "Matched rows: 903\n",
      "Match rate: 90.0%\n",
      "Merged dataframe shape: (903, 21)\n",
      "Columns in merged: ['Source_ledger', 'ISIN_CUSIP_ledger', 'Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'Amount1_ledger', 'Amount2_ledger', 'Signage_ledger', 'Ref1', 'Ref2', 'Trade_status_ledger', 'signed_amount_ledger', 'abs_amount', 'Source_statement']...\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 3: Match Transactions ====================\n",
    "print(\"Matching transactions...\")\n",
    "\n",
    "# Build match keys dynamically\n",
    "base_keys = [\n",
    "    \"Trade_Date\",\n",
    "    \"Currency\",\n",
    "    \"Tran_code\",\n",
    "    \"Quantity\",\n",
    "    \"abs_amount\"\n",
    "]\n",
    "\n",
    "match_keys = base_keys.copy()\n",
    "\n",
    "# Add optional keys if they exist\n",
    "if \"Ref1\" in ledger.columns and \"Ref1\" in statement.columns:\n",
    "    match_keys.append(\"Ref1\")\n",
    "    print(\"✓ Including Ref1 in match keys\")\n",
    "    \n",
    "if \"Ref2\" in ledger.columns and \"Ref2\" in statement.columns:\n",
    "    match_keys.append(\"Ref2\")\n",
    "    print(\"✓ Including Ref2 in match keys\")\n",
    "\n",
    "print(f\"\\nUsing match keys: {match_keys}\")\n",
    "\n",
    "merged = ledger.merge(\n",
    "    statement,\n",
    "    on=match_keys,\n",
    "    suffixes=(\"_ledger\", \"_statement\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(f\"\\nMatched rows: {len(merged)}\")\n",
    "print(f\"Match rate: {100 * len(merged) / len(ledger):.1f}%\")\n",
    "print(f\"Merged dataframe shape: {merged.shape}\")\n",
    "print(f\"Columns in merged: {list(merged.columns)[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729b4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data...\n",
      "  Processed 100/903 samples...\n",
      "  Processed 200/903 samples...\n",
      "  Processed 300/903 samples...\n",
      "  Processed 400/903 samples...\n",
      "  Processed 500/903 samples...\n",
      "  Processed 600/903 samples...\n",
      "  Processed 700/903 samples...\n",
      "  Processed 800/903 samples...\n",
      "  Processed 900/903 samples...\n",
      "\n",
      "✓ Created 903 training samples\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 4: Calculate Confidence & Create Training Data ====================\n",
    "def calculate_confidence(ledger_row, statement_row):\n",
    "    \"\"\"Calculate confidence score (0.0-1.0) based on match quality.\"\"\"\n",
    "    confidence = 1.0\n",
    "    \n",
    "    try:\n",
    "        if ledger_row.get(\"ISIN_CUSIP\") != statement_row.get(\"ISIN_CUSIP\"):\n",
    "            confidence -= 0.15\n",
    "        if str(ledger_row.get(\"Trade_Date\")) != str(statement_row.get(\"Trade_Date\")):\n",
    "            confidence -= 0.10\n",
    "        if ledger_row.get(\"Currency\") != statement_row.get(\"Currency\"):\n",
    "            confidence -= 0.20\n",
    "        if ledger_row.get(\"Quantity\") != statement_row.get(\"Quantity\"):\n",
    "            confidence -= 0.15\n",
    "        if abs(float(ledger_row.get(\"signed_amount\", 0)) - float(statement_row.get(\"signed_amount\", 0))) > 0.01:\n",
    "            confidence -= 0.20\n",
    "        if ledger_row.get(\"Ref1\", \"N/A\") != statement_row.get(\"Ref1\", \"N/A\"):\n",
    "            confidence -= 0.05\n",
    "        if ledger_row.get(\"Ref2\", \"N/A\") != statement_row.get(\"Ref2\", \"N/A\"):\n",
    "            confidence -= 0.05\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating confidence: {e}\")\n",
    "    \n",
    "    return max(0.0, min(1.0, confidence))\n",
    "\n",
    "def create_finetune_sample(ledger_row, statement_row):\n",
    "    \"\"\"Create a training sample for fine-tuning.\"\"\"\n",
    "    confidence = calculate_confidence(ledger_row, statement_row)\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these transactions and determine if they reconcile:\n",
    "\n",
    "Ledger Transaction:\n",
    "Source: {ledger_row.get('Source', 'Unknown')}\n",
    "Date: {ledger_row.get('Trade_Date', 'Unknown')}\n",
    "Currency: {ledger_row.get('Currency', 'Unknown')}\n",
    "Amount: {ledger_row.get('signed_amount', 0)}\n",
    "Quantity: {ledger_row.get('Quantity', 0)}\n",
    "Reference: {ledger_row.get('Ref1', 'N/A')}\n",
    "\n",
    "Statement Transaction:\n",
    "Source: {statement_row.get('Source', 'Unknown')}\n",
    "Date: {statement_row.get('Trade_Date', 'Unknown')}\n",
    "Currency: {statement_row.get('Currency', 'Unknown')}\n",
    "Amount: {statement_row.get('signed_amount', 0)}\n",
    "Quantity: {statement_row.get('Quantity', 0)}\n",
    "Reference: {statement_row.get('Ref1', 'N/A')}\n",
    "\n",
    "Question: Do these transactions match?\"\"\"\n",
    "    \n",
    "    assistant_response = f\"\"\"Yes, these transactions reconcile with confidence score {confidence:.2f}.\n",
    "Match Details:\n",
    "- Date: Match\n",
    "- Currency: Match\n",
    "- Amount: Match (accounting for signage differences)\n",
    "- Quantity: Match\n",
    "- References: Match\n",
    "Recommendation: RECONCILED\"\"\"\n",
    "    \n",
    "    formatted_text = f\"\"\"[INST] {user_prompt} [/INST] {assistant_response}\"\"\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "print(\"Creating training data...\")\n",
    "training_data = []\n",
    "for idx, (_, row) in enumerate(merged.iterrows()):\n",
    "    # Use .get() to safely access columns\n",
    "    ledger_row = {\n",
    "        \"Source\": str(row.get(\"Source_ledger\", \"Unknown\")),\n",
    "        \"Trade_Date\": str(row.get(\"Trade_Date\", \"Unknown\")),\n",
    "        \"Currency\": str(row.get(\"Currency\", \"Unknown\")),\n",
    "        \"signed_amount\": float(row.get(\"signed_amount_ledger\", 0)),\n",
    "        \"Quantity\": int(row.get(\"Quantity\", 0)),\n",
    "        \"Ref1\": str(row.get(\"Ref1\", \"N/A\")),\n",
    "        \"ISIN_CUSIP\": str(row.get(\"ISIN_CUSIP_ledger\", \"Unknown\"))\n",
    "    }\n",
    "    \n",
    "    statement_row = {\n",
    "        \"Source\": str(row.get(\"Source_statement\", \"Unknown\")),\n",
    "        \"Trade_Date\": str(row.get(\"Trade_Date\", \"Unknown\")),\n",
    "        \"Currency\": str(row.get(\"Currency\", \"Unknown\")),\n",
    "        \"signed_amount\": float(row.get(\"signed_amount_statement\", 0)),\n",
    "        \"Quantity\": int(row.get(\"Quantity\", 0)),\n",
    "        \"Ref1\": str(row.get(\"Ref1\", \"N/A\")),\n",
    "        \"ISIN_CUSIP\": str(row.get(\"ISIN_CUSIP_statement\", \"Unknown\"))\n",
    "    }\n",
    "    \n",
    "    training_data.append(create_finetune_sample(\n",
    "        pd.Series(ledger_row),\n",
    "        pd.Series(statement_row)\n",
    "    ))\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(merged)} samples...\")\n",
    "\n",
    "print(f\"\\n✓ Created {len(training_data)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28201ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training data...\n",
      "✓ Dataset saved to C:\\Users\\HarithaNagamalla\\Downloads\\reconciliation_training_data.jsonl\n",
      "\n",
      "Sample training data (first 200 chars):\n",
      "[INST] Analyze these transactions and determine if they reconcile:\n",
      "\n",
      "Ledger Transaction:\n",
      "Source: Bloomberg\n",
      "Date: 2022-05-07\n",
      "Currency: CAD\n",
      "Amount: -25633.43\n",
      "Quantity: 111\n",
      "Reference: 7GVUPC\n",
      "\n",
      "Statement Tr...\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 5: Save Training Data ====================\n",
    "print(\"Saving training data...\")\n",
    "\n",
    "output_path = r\"C:\\Users\\HarithaNagamalla\\Downloads\\reconciliation_training_data.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    for item in training_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Dataset saved to {output_path}\")\n",
    "print(f\"\\nSample training data (first 200 chars):\\n{training_data[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd57e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Hugging Face dataset...\n",
      "✓ Dataset size: 903\n",
      "✓ Sample text length: 579 characters\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 6: Create Dataset ====================\n",
    "print(\"Creating Hugging Face dataset...\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': [item['text'] for item in training_data]\n",
    "})\n",
    "\n",
    "print(f\"✓ Dataset size: {len(train_dataset)}\")\n",
    "print(f\"✓ Sample text length: {len(train_dataset[0]['text'])} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6780c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash attention not available, trying default attention: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [10:18<00:00, 206.29s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using default attention\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 7: Load Model & Tokenizer ====================\n",
    "print(\"Loading model and tokenizer...\")\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# 4-bit quantization config with CPU offloading for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading to prevent OOM\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with quantization - with tiered fallback strategy\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    print(\"✓ Using Flash Attention 2\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        print(f\"Flash attention not available, trying default attention: {e}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"✓ Using default attention\")\n",
    "    except Exception as e2:\n",
    "        print(f\"GPU loading failed, using CPU-only mode: {e2}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        quantization_config=bnb_config,print(f\"✓ Tokenizer loaded\")\n",
    "\n",
    "        device_map=\"cpu\",print(f\"✓ Model loaded: {MODEL_NAME}\")\n",
    "\n",
    "        trust_remote_code=True\n",
    "\n",
    "        model.enable_input_require_grads()  # Optimize gradient computation\n",
    "\n",
    "        print(\"✓ Using CPU-only mode (slower)\")\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring LoRA...\n"
     ]
    }
   ],
   "source": [
    "# ==================== STEP 8: Configure LoRA ====================\n",
    "print(\"Configuring LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"✓ Trainable params: {trainable_params:,} / {total_params:,}\")\n",
    "print(f\"✓ Trainable percentage: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec92665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 9: Tokenize Dataset ====================\n",
    "print(\"Tokenizing dataset...\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenized dataset size: {len(tokenized_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab077894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 10: Setup Training ====================\n",
    "print(\"Configuring training...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\HarithaNagamalla\\Downloads\\reconciliation_model\",\n",
    "    num_train_epochs=2,  # Reduced from 3 for memory efficiency\n",
    "    per_device_train_batch_size=1,  # Reduced from 4 to minimize GPU usage\n",
    "    gradient_accumulation_steps=4,  # Increased from 2 to maintain effective batch size\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,  # Reduced from 100\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=30,  # Reduced from 50\n",
    "    save_total_limit=1,  # Reduced from 2 to save disk space\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    seed=42,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "print(\"✓ Training config ready (optimized for GPU memory: batch_size=1, gradient_accumulation=4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 11: Train Model ====================\n",
    "print(\"Initializing trainer...\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING FINE-TUNING...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 12: Save Fine-tuned Model ====================\n",
    "print(\"\\nSaving fine-tuned model...\")\n",
    "\n",
    "model_save_path = r\"C:\\Users\\HarithaNagamalla\\Downloads\\reconciliation_model_finetuned\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"✓ Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 13: Test Inference ====================\n",
    "print(\"Loading model for inference...\")\n",
    "\n",
    "model_inference = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_save_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"[INST] Analyze these transactions and determine if they reconcile:\n",
    "\n",
    "Ledger Transaction:\n",
    "Source: Bloomberg\n",
    "Date: 2022-05-07\n",
    "Currency: CAD\n",
    "Amount: -25633.43\n",
    "Quantity: 111\n",
    "\n",
    "Statement Transaction:\n",
    "Source: Calypso\n",
    "Date: 2022-05-07\n",
    "Currency: CAD\n",
    "Amount: 25633.43\n",
    "Quantity: 111\n",
    "\n",
    "Question: Do these transactions match? [/INST]\"\"\"\n",
    "\n",
    "print(\"\\nGenerating prediction...\")\n",
    "inputs = tokenizer_inference(test_prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "outputs = model_inference.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "response = tokenizer_inference.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PREDICTION:\")\n",
    "print(\"=\"*80)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849da3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 14: Summary ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECONCILIATION FINE-TUNING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"✓ Processed {len(ledger)} ledger transactions\")\n",
    "print(f\"✓ Processed {len(statement)} statement transactions\")\n",
    "print(f\"✓ Matched {len(merged)} transaction pairs\")\n",
    "print(f\"✓ Generated {len(training_data)} training samples\")\n",
    "print(f\"✓ Fine-tuned model saved to: {model_save_path}\")\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"✓ Base Model: {MODEL_NAME}\")\n",
    "print(f\"✓ Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"✓ Training Time: ~30-60 minutes on GPU\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"1. Use the model for inference on new reconciliation tasks\")\n",
    "print(f\"2. Evaluate accuracy on a test dataset\")\n",
    "print(f\"3. Deploy as an API endpoint for production use\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
