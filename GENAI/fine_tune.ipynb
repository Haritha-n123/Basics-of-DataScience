{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e167a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "ledger = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_LDGR_Bloomberg_251125 1.csv\")\n",
    "statement = pd.read_csv(r\"C:\\Users\\HarithaNagamalla\\Downloads\\SRC_STMT_Calypso_251125 1.csv\")\n",
    "\n",
    "# Parse dates properly - use format='mixed' to handle varying formats\n",
    "ledger[\"Trade_Date\"] = pd.to_datetime(ledger[\"Trade_Date\"], format=\"mixed\", dayfirst=True)\n",
    "statement[\"Trade_Date\"] = pd.to_datetime(statement[\"Trade_Date\"], format=\"mixed\", dayfirst=True)\n",
    "\n",
    "# Normalize to date only for matching (remove time component)\n",
    "ledger[\"Trade_Date\"] = ledger[\"Trade_Date\"].dt.date\n",
    "statement[\"Trade_Date\"] = statement[\"Trade_Date\"].dt.date\n",
    "\n",
    "ledger[\"source_type\"] = \"ledger\"\n",
    "statement[\"source_type\"] = \"statement\"\n",
    "\n",
    "# Normalize signage\n",
    "def normalize_signage(x):\n",
    "    return x.strip().upper()\n",
    "\n",
    "ledger[\"Signage\"] = ledger[\"Signage\"].apply(normalize_signage)\n",
    "statement[\"Signage\"] = statement[\"Signage\"].apply(normalize_signage)\n",
    "\n",
    "# Create signed amount\n",
    "def signed_amount(row):\n",
    "    if row[\"Signage\"] in [\"DR\", \"D\"]:\n",
    "        return -row[\"Amount1\"]\n",
    "    elif row[\"Signage\"] in [\"CR\", \"C\"]:\n",
    "        return row[\"Amount1\"]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "ledger[\"signed_amount\"] = ledger.apply(signed_amount, axis=1)\n",
    "statement[\"signed_amount\"] = statement.apply(signed_amount, axis=1)\n",
    "\n",
    "# For matching, use absolute amounts (signage differences are normal between systems)\n",
    "ledger[\"abs_amount\"] = ledger[\"signed_amount\"].abs()\n",
    "statement[\"abs_amount\"] = statement[\"signed_amount\"].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff9f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEDGER COLUMNS:\n",
      "['Source', 'ISIN_CUSIP', 'Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'Amount1', 'Amount2', 'Signage', 'Ref1', 'Ref2', 'Trade_status', 'source_type', 'signed_amount', 'abs_amount']\n",
      "\n",
      "Ledger dtypes:\n",
      "Source            object\n",
      "ISIN_CUSIP        object\n",
      "Trade_Date        object\n",
      "Currency          object\n",
      "Tran_code         object\n",
      "Quantity           int64\n",
      "Amount1          float64\n",
      "Amount2          float64\n",
      "Signage           object\n",
      "Ref1              object\n",
      "Ref2              object\n",
      "Trade_status      object\n",
      "source_type       object\n",
      "signed_amount    float64\n",
      "abs_amount       float64\n",
      "dtype: object\n",
      "\n",
      "Ledger shape: (1003, 15)\n",
      "\n",
      "First few ledger rows:\n",
      "   Trade_Date Currency Tran_code  Quantity  signed_amount\n",
      "0  2022-05-07      CAD       DIV       111      -25633.43\n",
      "1  2025-05-27      CAD       BUY       896      -75545.67\n",
      "2  2024-03-27      GBP       INT       197       77472.69\n",
      "3  2024-09-20      GBP       BUY       288       84100.53\n",
      "4  2020-01-09      EUR       BUY       429       50337.47\n",
      "\n",
      "==================================================\n",
      "STATEMENT COLUMNS:\n",
      "['Source', 'ISIN_CUSIP', 'Trade_Date', 'Currency', 'Tran_code', 'Quantity', 'Amount1', 'Amount2', 'Signage', 'Ref1', 'Ref2', 'Trade_status', 'source_type', 'signed_amount', 'abs_amount']\n",
      "\n",
      "Statement dtypes:\n",
      "Source            object\n",
      "ISIN_CUSIP        object\n",
      "Trade_Date        object\n",
      "Currency          object\n",
      "Tran_code         object\n",
      "Quantity           int64\n",
      "Amount1          float64\n",
      "Amount2          float64\n",
      "Signage           object\n",
      "Ref1              object\n",
      "Ref2              object\n",
      "Trade_status      object\n",
      "source_type       object\n",
      "signed_amount    float64\n",
      "abs_amount       float64\n",
      "dtype: object\n",
      "\n",
      "Statement shape: (1000, 15)\n",
      "\n",
      "First few statement rows:\n",
      "   Trade_Date Currency Tran_code  Quantity  signed_amount\n",
      "0  2022-05-07      CAD       DIV       111       25633.43\n",
      "1  2025-05-27      CAD       BUY       896       75545.67\n",
      "2  2024-03-27      GBP       INT       197      -77472.69\n",
      "3  2024-09-20      GBP       BUY       288      -84100.53\n",
      "4  2020-01-09      EUR       BUY       429      -50337.47\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check columns and data types\n",
    "print(\"LEDGER COLUMNS:\")\n",
    "print(ledger.columns.tolist())\n",
    "print(\"\\nLedger dtypes:\")\n",
    "print(ledger.dtypes)\n",
    "print(\"\\nLedger shape:\", ledger.shape)\n",
    "print(\"\\nFirst few ledger rows:\")\n",
    "print(ledger[[\"Trade_Date\", \"Currency\", \"Tran_code\", \"Quantity\", \"signed_amount\"]].head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATEMENT COLUMNS:\")\n",
    "print(statement.columns.tolist())\n",
    "print(\"\\nStatement dtypes:\")\n",
    "print(statement.dtypes)\n",
    "print(\"\\nStatement shape:\", statement.shape)\n",
    "print(\"\\nFirst few statement rows:\")\n",
    "print(statement[[\"Trade_Date\", \"Currency\", \"Tran_code\", \"Quantity\", \"signed_amount\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ccb9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_confidence(ledger_row, statement_row):\n",
    "    \"\"\"\n",
    "    Calculate confidence score based on match quality.\n",
    "    Perfect match = 1.0, deductions for minor discrepancies.\n",
    "    \"\"\"\n",
    "    confidence = 1.0\n",
    "    \n",
    "    # Check for exact matches on key fields\n",
    "    if ledger_row[\"ISIN_CUSIP\"] != statement_row[\"ISIN_CUSIP\"]:\n",
    "        confidence -= 0.15\n",
    "    \n",
    "    if ledger_row[\"Trade_Date\"] != statement_row[\"Trade_Date\"]:\n",
    "        confidence -= 0.10\n",
    "    \n",
    "    if ledger_row[\"Currency\"] != statement_row[\"Currency\"]:\n",
    "        confidence -= 0.20\n",
    "    \n",
    "    if ledger_row[\"Quantity\"] != statement_row[\"Quantity\"]:\n",
    "        confidence -= 0.15\n",
    "    \n",
    "    # Check if amounts match (should match after abs normalization)\n",
    "    if abs(ledger_row[\"signed_amount\"] - statement_row[\"signed_amount\"]) > 0.01:\n",
    "        confidence -= 0.20\n",
    "    \n",
    "    # Check references\n",
    "    if ledger_row[\"Ref1\"] != statement_row[\"Ref1\"]:\n",
    "        confidence -= 0.05\n",
    "    \n",
    "    if ledger_row[\"Ref2\"] != statement_row[\"Ref2\"]:\n",
    "        confidence -= 0.05\n",
    "    \n",
    "    # Ensure confidence is between 0 and 1\n",
    "    return max(0.0, min(1.0, confidence))\n",
    "\n",
    "def create_finetune_sample(ledger_row, statement_row):\n",
    "    confidence = calculate_confidence(ledger_row, statement_row)\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "Ledger Transaction:\n",
    "{ledger_row.to_dict()}\n",
    "\n",
    "Statement Transaction:\n",
    "{statement_row.to_dict()}\n",
    "\n",
    "Task:\n",
    "Check if these transactions reconcile.\n",
    "Return:\n",
    "- matched (true/false)\n",
    "- reason\n",
    "- confidence (0-1)\n",
    "\"\"\"\n",
    "\n",
    "    assistant_response = {\n",
    "        \"matched\": True,\n",
    "        \"ledger_source\": ledger_row[\"Source\"],\n",
    "        \"statement_source\": statement_row[\"Source\"],\n",
    "        \"confidence\": round(confidence, 2),\n",
    "        \"reason\": (\n",
    "            \"Amounts, currency, trade date, transaction code, quantity \"\n",
    "            \"and references match across ledger and statement. \"\n",
    "            \"Signage difference handled via accounting rules.\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial reconciliation expert.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(assistant_response)}\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990156a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched rows: 903\n"
     ]
    }
   ],
   "source": [
    "match_keys = [\n",
    "    \"Trade_Date\",\n",
    "    \"Currency\",\n",
    "    \"Tran_code\",\n",
    "    \"Quantity\",\n",
    "    \"Ref1\",\n",
    "    \"Ref2\",\n",
    "    \"abs_amount\"  # Use absolute amount for matching\n",
    "]\n",
    "\n",
    "merged = ledger.merge(\n",
    "    statement,\n",
    "    on=match_keys,\n",
    "    suffixes=(\"_ledger\", \"_statement\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Matched rows:\", len(merged))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for _, row in merged.iterrows():\n",
    "\n",
    "    ledger_row = {\n",
    "        \"Source\": row[\"Source_ledger\"],\n",
    "        \"ISIN_CUSIP\": row[\"ISIN_CUSIP_ledger\"],\n",
    "        \"Trade_Date\": str(row[\"Trade_Date\"]),\n",
    "        \"Currency\": row[\"Currency\"],\n",
    "        \"Tran_code\": row[\"Tran_code\"],\n",
    "        \"Quantity\": row[\"Quantity\"],\n",
    "        \"signed_amount\": row[\"signed_amount_ledger\"],\n",
    "        \"Ref1\": row[\"Ref1\"],\n",
    "        \"Ref2\": row[\"Ref2\"],\n",
    "        \"Trade_status\": row[\"Trade_status_ledger\"]\n",
    "    }\n",
    "\n",
    "    statement_row = {\n",
    "        \"Source\": row[\"Source_statement\"],\n",
    "        \"ISIN_CUSIP\": row[\"ISIN_CUSIP_statement\"],\n",
    "        \"Trade_Date\": str(row[\"Trade_Date\"]),\n",
    "        \"Currency\": row[\"Currency\"],\n",
    "        \"Tran_code\": row[\"Tran_code\"],\n",
    "        \"Quantity\": row[\"Quantity\"],\n",
    "        \"signed_amount\": row[\"signed_amount_statement\"],\n",
    "        \"Ref1\": row[\"Ref1\"],\n",
    "        \"Ref2\": row[\"Ref2\"],\n",
    "        \"Trade_status\": row[\"Trade_status_statement\"]\n",
    "    }\n",
    "\n",
    "    training_data.append(\n",
    "        create_finetune_sample(\n",
    "            pd.Series(ledger_row),\n",
    "            pd.Series(statement_row)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be66ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"reconciliation_finetune.jsonl\", \"w\") as f:\n",
    "    for row in training_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144aba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: False\n",
      "GPU Count: 0\n",
      "Loading model in bfloat16 (FAST - no quantization overhead)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarithaNagamalla\\Documents\\Python and ML\\python_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HarithaNagamalla\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"  # Instruct version = better for tasks\n",
    "OUTPUT_DIR = \"./mistral_reconciliation_lora\"\n",
    "BATCH_SIZE = 2  # Reduced for memory\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ==================== FAST PATH: Load without Quantization ====================\n",
    "print(\"Loading model in bfloat16 (FAST - no quantization overhead)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,  # Native precision - much faster than quantization\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"  # Use flash attention for speed\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ==================== LoRA Configuration ====================\n",
    "# LoRA adapters only fine-tune ~50M parameters out of 7B\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                           # Smaller rank for speed\n",
    "    lora_alpha=16,                 # Smaller alpha\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"\\n========== LoRA Model Info ==========\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ==================== Tokenizer Setup ====================\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ==================== Prepare Dataset ====================\n",
    "print(\"Preparing dataset...\")\n",
    "\n",
    "def load_jsonl_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "raw_data = load_jsonl_dataset(\"reconciliation_finetune.jsonl\")\n",
    "\n",
    "# Create formatted text for training\n",
    "formatted_data = []\n",
    "for item in raw_data:\n",
    "    messages = item[\"messages\"]\n",
    "    formatted_text = \"\"\n",
    "    for msg in messages:\n",
    "        formatted_text += f\"{msg['role']}: {msg['content']}\\n\"\n",
    "    formatted_data.append({\"text\": formatted_text})\n",
    "\n",
    "# Save as temporary JSONL\n",
    "with open(\"temp_formatted.jsonl\", \"w\") as f:\n",
    "    for item in formatted_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"temp_formatted.jsonl\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_dataset['test'])}\")\n",
    "\n",
    "# ==================== Training Configuration ====================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",  # Faster than paged_adamw\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    bf16=True,  # Use bfloat16 for mixed precision\n",
    ")\n",
    "\n",
    "# ==================== Initialize Trainer ====================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ==================== Start Fine-tuning ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting FAST Mistral LoRA fine-tuning (no quantization)...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Training on {len(tokenized_dataset['train'])} samples\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==================== Save Model ====================\n",
    "print(f\"\\nSaving LoRA adapter to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/adapter\")\n",
    "\n",
    "print(\"✓ Fine-tuning complete!\")\n",
    "print(f\"✓ LoRA adapter saved to {OUTPUT_DIR}/adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Inference with Fine-tuned LoRA Model ====================\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "print(\"Loading fine-tuned LoRA model for inference...\")\n",
    "model_inference = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./mistral_reconciliation_lora/adapter\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(\"./mistral_reconciliation_lora/adapter\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"[INST] You are a financial reconciliation expert. Analyze these two transactions and determine if they match.\n",
    "\n",
    "Ledger: Source=Bloomberg, Date=2022-05-07, Currency=CAD, Amount=-25633.43, Quantity=111, RefID=REF123\n",
    "Statement: Source=Calypso, Date=2022-05-07, Currency=CAD, Amount=25633.43, Quantity=111, RefID=REF123\n",
    "\n",
    "Return: matched (true/false) and confidence (0-1) [/INST]\"\"\"\n",
    "\n",
    "print(\"\\nGenerating prediction...\")\n",
    "inputs = tokenizer_inference(test_prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "outputs = model_inference.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = tokenizer_inference.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PREDICTION:\")\n",
    "print(\"=\"*80)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 2: QLoRA Configuration ====================\n",
    "# LoRA adapters only fine-tune 1-2% of parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # LoRA rank\n",
    "    lora_alpha=32,                 # LoRA alpha\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 3: Tokenizer Setup ====================\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 4: Prepare Dataset ====================\n",
    "print(\"Preparing dataset...\")\n",
    "\n",
    "# Load JSONL file\n",
    "def load_jsonl_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Convert to dataset format\n",
    "raw_data = load_jsonl_dataset(\"reconciliation_finetune.jsonl\")\n",
    "\n",
    "# Create formatted text for training\n",
    "formatted_data = []\n",
    "for item in raw_data:\n",
    "    messages = item[\"messages\"]\n",
    "    formatted_text = \"\"\n",
    "    for msg in messages:\n",
    "        formatted_text += f\"{msg['role']}: {msg['content']}\\n\"\n",
    "    formatted_data.append({\"text\": formatted_text})\n",
    "\n",
    "# Save as temporary JSONL for dataset loading\n",
    "with open(\"temp_formatted.jsonl\", \"w\") as f:\n",
    "    for item in formatted_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"temp_formatted.jsonl\")\n",
    "\n",
    "# Split into train and validation\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Training samples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(tokenized_dataset['test'])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 5: Training Configuration ====================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 6: Initialize Trainer ====================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Step 7: Start Fine-tuning ====================\n",
    "print(\"\\nStarting fine-tuning with Mistral QLoRA...\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Training on {len(tokenized_dataset['train'])} samples\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==================== Step 8: Save Model ====================\n",
    "print(f\"\\nSaving model to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(\"✓ Fine-tuning complete!\")\n",
    "print(f\"✓ Model saved to {OUTPUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc89ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Inference and Testing ====================\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model\n",
    "MODEL_DIR = f\"{OUTPUT_DIR}/final_model\"\n",
    "\n",
    "print(\"Loading fine-tuned model for inference...\")\n",
    "model_inference = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer_inference = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer_inference.pad_token = tokenizer_inference.eos_token\n",
    "\n",
    "# Test with a sample reconciliation query\n",
    "test_prompt = \"\"\"You are a financial reconciliation expert.\n",
    "\n",
    "Ledger Transaction:\n",
    "{'Source': 'Bloomberg', 'ISIN_CUSIP': 'US0378331005', 'Trade_Date': '2022-05-07', 'Currency': 'CAD', 'Tran_code': 'DIV', 'Quantity': 111, 'signed_amount': -25633.43, 'Ref1': 'REF123', 'Ref2': 'REF456', 'Trade_status': 'SETTLED'}\n",
    "\n",
    "Statement Transaction:\n",
    "{'Source': 'Calypso', 'ISIN_CUSIP': 'US0378331005', 'Trade_Date': '2022-05-07', 'Currency': 'CAD', 'Tran_code': 'DIV', 'Quantity': 111, 'signed_amount': 25633.43, 'Ref1': 'REF123', 'Ref2': 'REF456', 'Trade_status': 'SETTLED'}\n",
    "\n",
    "Task:\n",
    "Check if these transactions reconcile.\n",
    "Return:\n",
    "- matched (true/false)\n",
    "- reason\n",
    "- confidence (0-1)\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer_inference(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_inference.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = tokenizer_inference.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ffa1f5",
   "metadata": {},
   "source": [
    "<!-- ## Mistral QLoRA Fine-tuning Summary\n",
    "\n",
    "## What was done\n",
    "✓ **4-bit Quantization**: Model compressed to 4-bit for GPU memory efficiency  \n",
    "✓ **LoRA Adapters**: Fine-tuned only 1-2% of model parameters (~50M out of 7B)  \n",
    "✓ **Memory-Efficient Training**: Used gradient checkpointing and paged optimizer  \n",
    "✓ **Dataset Split**: 90% training, 10% validation on 903 reconciliation samples  \n",
    "✓ **Training Config**: 3 epochs, batch size 4, learning rate 2e-4, cosine scheduler  \n",
    "\n",
    "## Model Performance Metrics\n",
    "- Monitor in tensorboard: `tensorboard --logdir ./mistral_reconciliation_qora/logs`\n",
    "- Key metrics: Training loss, validation loss, perplexity\n",
    "- Expected convergence: Loss should decrease steadily over epochs\n",
    "\n",
    "## Output Structure\n",
    "```\n",
    "mistral_reconciliation_qora/\n",
    "├── final_model/              # Fine-tuned weights + config\n",
    "│   ├── adapter_config.json\n",
    "│   ├── adapter_model.bin\n",
    "│   ├── config.json\n",
    "│   └── tokenizer_config.json\n",
    "├── checkpoint-*/             # Intermediate checkpoints\n",
    "└── logs/                      # TensorBoard logs\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "1. **Inference**: Test the model on new reconciliation samples\n",
    "2. **Evaluation**: Assess accuracy vs. baseline on held-out test set\n",
    "3. **Deployment**: Merge adapters with base model for production\n",
    "4. **Fine-tuning**: Add more data or adjust hyperparameters\n",
    "\n",
    "## Merge Adapters with Base Model (Optional)\n",
    "```python\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./mistral_reconciliation_qora/final_model\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./mistral_merged\")\n",
    "``` -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96132ec9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
